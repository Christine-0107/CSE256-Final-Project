The TextVQA (Text Visual Question Answering) task requires the model to read and understand the visual and textual information within images to respond to presented questions. It requires the use of information from three modalities: question, image vision, and scene text. Among them, the scene text information in the image is crucial for correctly understanding the scene and answering questions. Existing TextVQA models indiscriminately input all texts recognized by the OCR system into subsequent models. Redundant text will interfere with answer generation and affect the accuracy of answer prediction.
In order to solve the above problems, I propose a TCM (Text Correlation Model) based on text correlation prediction. The model consists of a feature extraction module, an STA (Scene Text Augmenter) and an answer generation module. TCM initially processes the extracted question, visual, and textual features through the STA to capture key textual information relevant to specific question-answer pairs. Subsequently, it is integrated with the question and visual information and fed into the Large Language Model to produce predictive outcomes. The STA, as introduced in the report, is capable of predicting the relevance between scene text and both the question and image. It utilizes the multi-head attention mechanism to select scene text most pertinent to the question and image, thereby enhancing the modelâ€™s focus on critical textual information. Experimental results on two datasets validate the effectiveness of the proposed method.
